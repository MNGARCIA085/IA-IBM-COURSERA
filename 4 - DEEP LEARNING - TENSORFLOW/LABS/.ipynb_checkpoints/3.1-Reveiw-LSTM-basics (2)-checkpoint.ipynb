{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='blue'><center> Redes neuronales recurrentes en deep learning</center></font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos los conceptos del modelo LSTM (Long Short-Term Memory), un refinamiento del modelo original de redes neuronales recurrentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objetivos<h3>    \n",
    "<h5> 1. Aprender el modelo Long Short-Term</h5>\n",
    "<h5> 2. Stacked LTSM (LSTM apilado)</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Tabla de contenido</h2>\n",
    "<ol>\n",
    "    <li><a href=\"#intro\">Introducción</a></li>\n",
    "    <li><a href=\"#long_short_term_memory_model\">Modelo Long Short-Term Memory</a></li>\n",
    "    <li><a href=\"#ltsm\">LTSM</a></li>\n",
    "    <li><a href=\"#stacked_ltsm\">Stacked LTSM</a></li>\n",
    "</ol>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a> \n",
    "\n",
    "<h2>Introducción</h2>\n",
    "    \n",
    "Las redes neuronales recurrentes son modelos de deep learning con estructuras simples y un mecanismo de realimentación incorporado, o, en palabras diferentes, la salida de una capa es agregada a la siguiente entrada y realimentada a la misma capa.\n",
    "\n",
    "La red neuronal recurrente es un tipo especializado de red neuronal que resuelve el problema de **mantener el contexto para datos secuenciales** -- tales como el tiempo, stocks, genes, etc. En cada paso iterativo, la unidad de procesamiento toma una entrada y el estado actual de la red, y produce una salida y un nuevo estado que es realimentado en la red.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/v7p90neiaqghmpwawpiecmz9n7080m59.png\">\n",
    "\n",
    "<center><i>Representación de una red neuronal recurrente</i></center>\n",
    "<br><br>\n",
    "Sin embargo, **este modelo tiene algunos problemas**. Es computacionalmente caro mantener el estado de un gran conjunto de unidades, incluso más si es sobre un período largo de tiempo. Adicionalmente, las redes recurrentes son muy sensibles a cambios en sus parámetros. Como tal, son propensas a diferentes problemas con el optimizador del gradiente descendente -- o bien crece exponencialmente (exploding gradient) o bien cae a un valor cercano a 0 (desvanecimiento del gradiente); estos problemas dañan mucho la capacidad de aprendizaje del modelo.\n",
    "\n",
    "Para resolver estos problemas Hochreiter y Schmidhuber publicaron un paper en 1997 describiendo una forma de mantener la información sobre un largo período de tiempo y adicionalmente resolver la hipersensibilidad a los cambios en los parámetros, haciendo de la backpropagation a través de la red recurrente más viable. Este método se llama LSTM.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"long_short_term_memory_model\"></a>\n",
    "\n",
    "<h2>Long Short-Term Memory Model</h2>\n",
    "\n",
    "LSTM es una abstracción acerca de cómo funciona la memoria de la computadora. Está \"empaquetado\" con cualqquier unidad de procesamiento implementada en la red recurrente, aunque fuera de su flujo, y es responsable de mantener, leer y dar información de salida para el modelo. La forma en que funciona es simple: tiene una unidad lineal, que es la propia celda de información, rodeada por 3 puertas lógicas responsables de mantener los datos. Una puerta es para ingresar datos en la celda de información, otra es para enviar datos desde la puerta de entrada y otra es para mantener u olvidar los datos dependiendo de las necesidades de la red.\n",
    "\n",
    "Gracias a esto, no solo resuelve el problema de mantener los estados, porque la red puede elegir olvidar los datos cuando la información no es necesaria, sino también los referidos al gradiente, ya que las puertas lógicas tienen derivadas \"agradables\".\n",
    "\n",
    "<h3>Arquitectura LSTM</h3>\n",
    "\n",
    "LSTM es compuesto por una unidad lineal rodeada por 3 puertas lógicas. El nombre de estas puertas varía en la literatura, pero los más usuales son:\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>\"Input\" o \"Write\" Gate, que maneja la escritura de datos en la celda de información</li>\n",
    "    <li>\"Output\" o \"Read\" Gate, que maneja el enviar los datos de vuelta a la red recurrente</li>\n",
    "    <li>\"Keep\" o \"Forget\" Gate, que maneja el mantenimiento y modificación de los datos almacenados en la celda de información</li>\n",
    "</ul>\n",
    "<br>\n",
    "<img src=\"https://ibm.box.com/shared/static/zx10duv5egw0baw6gh2hzsgr8ex45gsg.png\" width=\"720\"/>\n",
    "<center><i>Diagrama de la unidad LSTM</i></center>\n",
    "<br><br>\n",
    "\n",
    "Las 3 puertas son la pieza central de la unidad LSTM. Las puertas, cuando son activadas por la red, realizan sus respectivas funciones. Por ejemplo, la puerta de entrada escribirá los datos que se le pasen en la celda de información, la de salida retornará los datos que estén en la celda de información y la de mantener mantendrá los datos en la célula de información. Estas puertas son analógicas y multiplicativas, y, como tales, pueden modificar los datos basadas en la señal que se envía.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Por ejemplo, un flujo usual de operaciones en la unidad LSTM es: Primero, la keep gate tiene que decidir si mantener u olvidar los datos actualmente almacenados en memoria. Recibe tanto la entrada como el estado de la red recurrente y lo pasa a través de la función de activación sigmoide. Si $K_t$ tiene el valor de 1 significa que la unidad LSTM debe mantener los datos perfectamente almacenados y si $K_t$ vale 0 debe olvidarlos por completo. Considere $S_{t-1}$ como el estado de entrada (previo), $x_t$ como la entrada, y  $W_k$ y $B_k$ como los pesos y sesgos para la keep gate. Considere $Old_{t-1}$ como los datos previamente almacenados en memoria. Lo que sucede puede resumirse en la siguiente ecuación:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"4\"><strong>\n",
    "$$K_t = \\\\sigma(W_k \\times [S_{t-1}, x_t] + B_k)$$\n",
    "\n",
    "$$Old_t = K_t \\\\times Old_{t-1}$$\n",
    "</strong></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Puede ver que $Old_{t-1}$ fue mutiplicado por ($K_t$) -- este valor se escribe en la célula de memoria.\n",
    "\n",
    "<br>\n",
    "Luego, la entrada y el estado son pasados a la puerta de entrada, donde hay otra sigmoide aplicada. Concurrentemente, la entrada es procesada como es habitual por cualquiera sea la unidad de procesamiento implementada en la red y luego multiplicada por la sigmoide, resultando $I_t$. Considere $W_i$ y $B_i$ como los pesos y sesgo para la puerta de entrada, y $C_t$ el resultado del procesamiento de las entradas por la red recurrente.\n",
    "<br><br>\n",
    "\n",
    "<font size=\"4\"><strong>\n",
    "$$I_t = \\\\sigma(W_i\\times[S_{t-1},x_t]+B_i)$$\n",
    "\n",
    "$$New_t = I_t \\\\times C_t$$\n",
    "</strong></font>\n",
    "\n",
    "<br>\n",
    "$New_t$ son los nuevos datos para ser ingresados en la celda de memoria. Esto es luego sumando a cualquier valor haya almacenado en memoria.\n",
    "<br><br>\n",
    "\n",
    "<font size=\"4\"><strong>\n",
    "$$Cell_t = Old_t + New_t$$\n",
    "</strong></font>\n",
    "\n",
    "<br>\n",
    "Ahora tenemos los datos candidatos que se mantendrán en la celda de memoria. La conjunción de las puertas Keep e Input funcionan de una forma análoga, haciendo posible mantener parte de los datos viejos y sólo una parte de los nuevos. Considere sin embargo lo que sucedería si la puerta Forget fuese establecida en 0 y la Input en 1:\n",
    "<br><br>\n",
    "\n",
    "<font size=\"4\"><strong>\n",
    "$$Old_t = 0 \\\\times Old_{t-1}$$\n",
    "\n",
    "$$New_t = 1 \\\\times C_t$$\n",
    "\n",
    "$$Cell_t = C_t$$\n",
    "</strong></font>\n",
    "\n",
    "<br>\n",
    "Los datos viejos serán completamente olvidados y los nuevos sobrescritos por completo.\n",
    "\n",
    "Las puerta de salida trabaja de forma similar. Para decidir qué dar como salida, tomamos la entrada y el estado y los hacemos pasar por la sigmoide. Los contenidos de nuestra celda de memoria son pasados a una Tanh para asociarlos a un valor entre -1 y 1. Sean $W_o$ y $B_o$ los pesos y sesgos para la puerta de salida.\n",
    "<br>\n",
    "<font size=\"4\"><strong>\n",
    "$$O_t = \\\\sigma(W_o \\times [S_{t-1},x_t] + B_o)$$\n",
    "\n",
    "$$Output_t = O_t \\\\times tanh(Cell_t)$$\n",
    "</strong></font>\n",
    "<br>\n",
    "\n",
    "$Output_t$ es lo que da como salida la red recurrente.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img width=\"384\" src=\"https://ibm.box.com/shared/static/rkr60528r3mz2fmtlpah8lqpg7mcsy0g.png\">\n",
    "<center><i>La función logística graficada</i></center>\n",
    "<br><br>\n",
    "Las 3 puertas son logísticas. La razón para esto es que es sencillo hacer propagación hacia atrás a través de ellas, y, como tales, es posible para el modelo aprender exactamente cómo se supone se debe usar esta estructura. Es una de las razones por la cuales la arquiectura LSTM es muy fuerte. Adicionalmente, esto resuelve los problemas del gradiente al ser capaz de manipular valores a través de las propias puertas -- pasando las entradas y salidas a través de las puertas, tenemos ahora una función fácilmente derivable modificando nuestras entradas.\n",
    "\n",
    "En lo que se refiere al problema de almacenar demasiados estados en memoria sobre un período largo de tiempo, LSTM lo maneja a la perfección manteniendo solamente la información necesaria.\n",
    "\n",
    "Así, las LSTM brindan una solución muy elegante a ambos problemas.\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instructions\"></a> \n",
    "\n",
    "<h2>Instrucciones</h2>\n",
    "    \n",
    "Instalamos las librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install grpcio==1.24.3\n",
    "#!pip install tensorflow==2.2.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"ltsm\"></a>\n",
    "\n",
    "<h2>LSTM</h2>\n",
    "Crearemos una LSTM pequeña para entender mejor su arquitectura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Importamos los módulos necesarios. Adicionalmente, podemos importar directamente <b><code>tensorflow.keras.layers</code></b>, que incluye la función para construir RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "if not tf.__version__ == '2.2.0-rc1': #2.2.0-rc0\n",
    "    print(tf.__version__)\n",
    "    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerde reiniciar el kernel para que los cambios tomen efecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Queremos crear una red con una única LSTM. Tenemos que pasar 2 elementos a LSTM, <b> prv_output </b> y <b> prv_state </b>, llamados, <b> h </b> y <b> c </b>. Por lo tanto, inicializamos un vector de estado, <b> state </b>. Aquí, <b> state </b> es una tupla con 2 elementos, cada uno tiene un tamaño de [1 x 4], uno para pasar prv_output al siguiente paso de tiempo y otro para pasar prv_state a la siguiente marca de tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
    "\n",
    "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "lstm = tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n",
    "\n",
    "lstm.states=state\n",
    "\n",
    "print(lstm.states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Podemos ver que el estado tiene 2 partes, el nuevo estado c, y la salida h. Chequeemos la salida otra vez:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Definamos una muestra de entrada. En este ejmplo, batch_size = 1 y features (características) = 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "\n",
    "batch_size = 1\n",
    "sentence_max_length = 1\n",
    "n_features = 6\n",
    "\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "\n",
    "inputs = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Ahora podemos pasarle la entrada a lstm_cell y chequear el nuevo estado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state = lstm(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :  tf.Tensor([1 1 4], shape=(3,), dtype=int32)\n",
      "Memory :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "Carry state :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('Output : ', tf.shape(output))\n",
    "\n",
    "print('Memory : ',tf.shape(final_memory_state))\n",
    "\n",
    "print('Carry state : ',tf.shape(final_carry_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "<a id=\"stacked_ltsm\"></a>\n",
    "<h2>Stacked LSTM</h2>\n",
    "Qué pasa si queremos tener una RNN que consista de LSTM apiladas? Por ejemplo con 2 capas. En este caso, la salida de la primera capa será la entrada de la segunda.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Creamos el stacked LSTM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la primera capa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
    "cell1 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_1)\n",
    "cells.append(cell1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la segunda capa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
    "cell2 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_2)\n",
    "cells.append(cell2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear una LSTM multi-capas usamos la función <b>tf.keras.layers.StackedRNNCells</b>,que toma múltiples LSTM individuales y crear un modelo LSTM apilado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm =  tf.keras.layers.StackedRNNCells(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Ahora creamos la RNN a partir de <b>stacked_lstm</b>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lstm_layer= tf.keras.layers.RNN(stacked_lstm ,return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Digamos que la secuencia de entrada tiene longitud 3 y la dimensionalidad de las entradas es 6. La entrada debe ser un tensor de la forma [batch_size, max_time, dimension], en nuestro caso será (2,3,6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
    "sample_input\n",
    "\n",
    "batch_size = 2\n",
    "time_steps = 3\n",
    "features = 6\n",
    "new_shape = (batch_size, time_steps, features)\n",
    "\n",
    "x = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "ahora podemos mandar nuestra entrada a la red y chequear la salida:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state  = lstm_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :  tf.Tensor([2 3 5], shape=(3,), dtype=int32)\n",
      "Memory :  tf.Tensor([2 2 4], shape=(3,), dtype=int32)\n",
      "Carry state :  tf.Tensor([2 2 5], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('Output : ', tf.shape(output))\n",
    "\n",
    "print('Memory : ',tf.shape(final_memory_state))\n",
    "\n",
    "print('Carry state : ',tf.shape(final_carry_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que la salida es de la forma (2,3,5), que corresponde a 2 lotes, 3 elementos en nuestra secuencia y la dimensionalidad de la salida, que es 5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
