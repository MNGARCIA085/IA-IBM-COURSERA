{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TAREA FINAL </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==2.4.5\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8 MB 10 kB/s s eta 0:00:01��█████████████▏              | 117.0 MB 68.9 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=373e89566c32526921d7b46f250a0b4677e9bfc4a987153ce9e455a39be15355\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarguemos el dataset y creemos un dataframe.\n",
    "\n",
    "[https://developer.ibm.com/exchanges/data/all/jfk-weather-data/](https://developer.ibm.com/exchanges/data/all/jfk-weather-data?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-22 21:49:37--  https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\n",
      "Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.45.183.81, 23.45.183.130, 23.45.183.80, ...\n",
      "Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.45.183.81|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3509993 (3.3M) [application/x-tar]\n",
      "Saving to: ‘noaa-weather-data-jfk-airport.tar.gz’\n",
      "\n",
      "noaa-weather-data-j 100%[===================>]   3.35M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-02-22 21:49:37 (123 MB/s) - ‘noaa-weather-data-jfk-airport.tar.gz’ saved [3509993/3509993]\n",
      "\n",
      "noaa-weather-data-jfk-airport/jfk_weather.csv\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -rf noaa-weather-data-jfk-airport*\n",
    "\n",
    "# download the file containing the data in CSV format\n",
    "!wget https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\n",
    "\n",
    "# extract the data\n",
    "!tar xvfz noaa-weather-data-jfk-airport.tar.gz noaa-weather-data-jfk-airport/jfk_weather.csv\n",
    "    \n",
    "# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('noaa-weather-data-jfk-airport/jfk_weather.csv')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset contiene algunos valores null, por tanto la inferencia del esquema no funcionará apropiadamente para todas las columnas. Además una columna tiene caracteres finales. Por tanto, debemos empezar limpiando el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from pyspark.sql.functions import translate, col\n",
    "\n",
    "df_cleaned = df \\\n",
    "    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n",
    "    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n",
    "    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n",
    "    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n",
    "\n",
    "df_cleaned =   df_cleaned \\\n",
    "                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n",
    "                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n",
    "\n",
    "df_filtered = df_cleaned.filter(\"\"\"\n",
    "    HOURLYWindSpeed <> 0\n",
    "    and HOURLYWindSpeed IS NOT NULL\n",
    "    and HOURLYWindDirection IS NOT NULL\n",
    "    and HOURLYStationPressure IS NOT NULL\n",
    "    and HOURLYPressureTendency IS NOT NULL\n",
    "    and HOURLYPrecip IS NOT NULL\n",
    "    and HOURLYRelativeHumidity IS NOT NULL\n",
    "    and HOURLYDRYBULBTEMPC IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos predecir el valor de una columna basados en otras. A veces es útil imprimir la matriz de correlación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.25478863, -0.26171147, -0.01324305],\n",
       "       [ 0.25478863,  1.        , -0.13377466, -0.18145395],\n",
       "       [-0.26171147, -0.13377466,  1.        , -0.05821663],\n",
       "       [-0.01324305, -0.18145395, -0.05821663,  1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYWindDirection\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\"],\n",
    "                                  outputCol=\"features\")\n",
    "df_pipeline = vectorAssembler.transform(df_filtered)\n",
    "from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df_pipeline,\"features\").head()[0].toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verse que HOURLYWindSpeed y HOURLYWindDirection tienen una correlación de 0.25478863 mientras que HOURLYWindSpeed y HOURLYStationPressure tienen una correlación de -0.26171147.\n",
    "Ya que es un problema de aprendizaje supervisado, dividamos nuestros datos en entrenamiento (80%) y test (20%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df_filtered.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reutilizamos nuestro pipeline de ingeniería de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\n",
    "                                    \"HOURLYWindDirection\",\n",
    "                                    \"ELEVATION\",\n",
    "                                    \"HOURLYStationPressure\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para evaluar la performance de nuestra regresión. Utilizamos RMSE (Root Mean Squared Error); mientras más chico mejor..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(prediction):\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(prediction)\n",
    "    print(\"RMSE on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutemos un modelo de regresión lineal para construir nuestra baseline (línea base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 3.15699\n"
     ]
    }
   ],
   "source": [
    "#LR1\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features', maxIter=100, regParam=0.0, elasticNetParam=0.0) #features_norm x features\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos Gradient Boosted Tree Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 5.1145\n"
     ]
    }
   ],
   "source": [
    "#GBT1\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cambiemos de tema. Anteriormente, intentamos predecir HOURLYWindSpeed, pero ahora predecimos HOURLYWindDirection. Para convertir esto en un problema de clasificación, discretizamos el valor usando el Bucketizer. La nueva característica se llama HOURLYWindDirectionBucketized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\n",
    "encoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para medir la performance. Aquí usamos la medida de precisión que nos da la fracción de muestras clasificadas correctamente. 0 es malo y 1 es bueno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(prediction):\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
    "    accuracy = mcEval.evaluate(prediction)\n",
    "    print(\"Accuracy on test data = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la línea base, usamos regresión logística:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.692922\n"
     ]
    }
   ],
   "source": [
    "#LGReg1\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n",
    "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "classification_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentemos otros algoritmos a ver si mejora la performance.También es importante ajustar otros parámetros como los parámetros de algoritmos individuales (por ejemplo, número de árboles para RandomForest) o parámetros en la tubería de ingeniería de características, por ejemplo Relación de división de tren / prueba, normalización, agrupamiento,…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.721233\n"
     ]
    }
   ],
   "source": [
    "#RF1\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=30) # de 30 a 10\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "classification_metrics(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.73242\n"
     ]
    }
   ],
   "source": [
    "#GBT2\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "classification_metrics(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
