{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font color='blue'>MODELOS PRE-ENTRENADOS</font></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Objetivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Veremos cómo aprovechar modelos pre-entrenados para construir clasificadores en lugar de hacer un modelo desde cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Tabla de contenido\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Importando librerías y paquetes</a>\n",
    "2. <a href=\"#item32\">Descargando los datos</a>  \n",
    "3. <a href=\"#item33\">Definiendo constantes globales</a>  \n",
    "4. <a href=\"#item34\">Construyendo instanciasImageDataGenerator</a>  \n",
    "5. <a href=\"#item35\">Compilando y ajustando el modelo</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Importando librerías y paquetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Importamos el módulo ImageDataGenerator ya que lo aprovecharemos para entrenar nuestro modelo en lotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Importamos Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Aprovecharemos el modelo ResNet50 para construir nuestro clasificador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Descargando los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('concrete_data_week3.zip', <http.client.HTTPMessage at 0x1b02dbaee20>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
    "\n",
    "import urllib.request\n",
    "url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip'\n",
    "filename = 'concrete_data_week3.zip'\n",
    "urllib.request.urlretrieve(url, filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Ahora verá en la carpeta apropiada el archivo _concrete_data_week3.zip_. Lo descomprimimos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('concrete_data_week3.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Ahora verá la carpeta _concrete_data_week3_. En ella hay 2 carpetas: _train_ y _valid_. Cada una contiene 2 subcarpetas: _positive_ y _negative_ donde negatve contiene las imágenes sin grietas y positive las que sí contienen grietas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Definiendo constantes globales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Definiremos constantes que usaremos en el resto de la notebook:\n",
    "\n",
    "1.  Estamos tratando con 2 clases, por lo que _num_classes_ es 2. \n",
    "2.  El modelo ResNet50 fue construido y entrenado usando imágenes de tamaño (224 x 224). Por tanto debemos cambiar el tamaño de las imágenes de (227x227) a (224x224).\n",
    "3. Para entrenar y validar el modelo usaremos lotes de 100 imágenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construyendo instancias ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Para instanciar una instancia de ImageDataGenerator estableceremos el argumento **preprocessing_function** a _preprocess_input_ que importamos de **keras.applications.resnet50** para preprocesar nuestras imágenes de la misma forma que fueron pre-procesadas cuando se entrenó el modelo ResNet50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Usaremos el método  _flow_from_directory_ para obtener las imágenes entrenadas como sigue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Repetimos pero ahora con las imágenes de validación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construir, compilar y ajustar el modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Comenzamos construyendo el modelo. Utilizaremos el modelo Sequential de Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Agregamos el modelo ResNet50 pre-entrenado a nuestro modelo. Sin embargo, tenga en cuenta que no queremos incluir la capa superior o la capa de salida del modelo previamente entrenado, queremos definir nuestra propia capa de salida y entrenarla para que esté optimizada para nuestro conjunto de imágenes. Para dejar fuera la capa de salida del modelo pre-entrenado, estableceremos el argumento _include_top_ en **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 24s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Luego, definimos nuestra capa de salida como una capa **Dense**, que consiste de 2 nodos y usa la función **Softmax** como función de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Puede acceder a las capas del modelo usando el atributo _layers_ de nuestro objeto modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.training.Model at 0x1b04d43c580>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1b04b603940>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Puede ver que nuestro modelo está compuesto por 2 capas. La primera es la perteneciente al ResNet50, y la segunda es la capa Dense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Puede acceder a las capas del ResNet50 ejecutando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1b0486621c0>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x1b048662b20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b047600880>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048651c40>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04881e4c0>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x1b0487f8640>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1b0487e13a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04887fd90>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048898190>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b0488e7b20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b0488d69a0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b0488d64c0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048970580>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04886a070>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048936b20>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04886abe0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048982a60>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b048996a30>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b0489888e0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048988a00>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048988970>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b0489f42b0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b0489e6760>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b0489e68b0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048a4b250>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048a40850>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048a842b0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b048aa3250>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048a95340>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048a954c0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048a95700>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048af8700>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048aee5e0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048aeed00>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048b50640>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048b446d0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048b87fa0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b048ba5640>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048b986d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048bf3d30>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048c00b50>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b048c54610>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048c47520>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048c47c10>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049c7d5e0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b048b98730>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049c70be0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b048b988b0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049cb6760>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b049cd15e0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049cc5970>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049cc5b20>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049cc5670>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049d2ab20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049d1e7c0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049d1e190>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049db3460>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049d7caf0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049dc5a90>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b049e0b460>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049dd2af0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049dca070>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049dcabe0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049e35070>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049e23df0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049e68af0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049e8f040>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049e7af70>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049ec6fa0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b049ee2160>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049ed1f70>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049ede160>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049ede2b0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049f3c490>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049f31820>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049f31a90>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049f92430>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049f868e0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049fcbca0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b049fea400>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b049fdb7f0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a035d30>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a041910>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a0973d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a089760>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a0899d0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a0ef3a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b049fdb880>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a0e3850>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b049fdb460>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a126b50>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04a144370>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a138760>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a1387f0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a1383d0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a1ce280>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a190850>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a190e80>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a1f57f0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a1e8940>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a22ba60>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04a24b7c0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a23f8e0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a23fa60>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a23fc40>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a2a5130>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a299b50>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a299a00>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a2f8c70>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a2ecac0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a33cc70>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04a353c40>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a342610>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a342fa0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a342940>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a39bbb0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a3a3610>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a3ae220>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a406190>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a3f0f40>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a43eaf0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04a45c190>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04a449fd0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04a4506d0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04a4504f0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04aca4610>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04ac966a0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04ac96c10>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04acf7580>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04acef970>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04ad33fd0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04ad50550>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04ad44940>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04ad44a90>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04ad449d0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b248550>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04ad9dbe0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b211ac0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b270a30>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b262880>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b2b4a00>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04b2c79d0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b2bb880>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b30e370>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b554100>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b8179a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b806190>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b806160>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b86c970>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b2bb9a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b85d820>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b2bb910>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b8b0970>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04b8c4940>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04b8b6d60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04b8b6910>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04b8b6880>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04bc70070>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04bc5ceb0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04bc5c7f0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04cf07190>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04cef3e20>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04cf45e20>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04cf61160>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04cf4abe0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04cf5d220>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04cf9daf0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04cfb6340>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04cfaa6d0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04cfaa940>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04d3ae2e0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b04d3a15b0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1b04d3e7460>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x1b04d4042b0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x1b04d3f9580>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x1b04d3f93a0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Como el modelo ResNet50 ya ha sido entrenado, queremos decirle a nuestro modelo que no se moleste en entrenar la parte del ResNet50; para ello ejecutamos lo siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Usando el atributo _summary_ del modelo podemos ver cuántos parámetros necesitamos optimizar para entrenar la capa de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Compilamos el modelo utilizando el optimizador **adam**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Antes de comenzar el proceso de entrenamiento, con ImageDataGenerator, necesitamos definir cuántos pasos componen un epoch. Típicamente, es el número de imágenes dividido por el tamaño del lote. Por tanto, definimos nuestros pasos por epoch como sigue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Estamos listos para entrenar el modelo. A diferencia del deep learning convencional, donde los datos no se transmiten desde un directorio, con ImageDataGenerator donde los datos se aumentan en lotes, usamos el método **fit_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-172e67583a70>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2\n",
      "301/301 [==============================] - 2009s 7s/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 0.0066 - val_accuracy: 0.9985\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 2015s 7s/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0043 - val_accuracy: 0.9991\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Ahora que el modelo fue entrenado, puede usarlo para comenzar a clasificar imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Como el entrenamiento puede demorar mucho con modelos de deep learning, siempre es una buena idea guardar el modelo una vez se completó el entrenamiento si cree que lo utilizará luego. Guardémoslo entonces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Se generó así el archivo _classifier_resnet_model.h5_.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
